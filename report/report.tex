\documentclass[a4paper]{article}
\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[export]{adjustbox}
\usepackage{enumerate}
\usepackage{url}
\usepackage{authblk}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[linesnumbered,ruled]{algorithm2e}

\graphicspath{{plots/}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\makeatletter
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
\makeatother

\DeclareMathSizes{10}{10}{10}{10}

\begin{document}

    \newgeometry{left=4cm,right=4cm,top=4cm,bottom=6cm}
	\begin{titlepage}
	    \begin{center}
	        \vspace*{1cm}
	        
	       	\huge{\textsc{Partial Multi View Clustering}}\\
			\Large{\textsc{Via Non Negative Matrix Factorization}}	

	        \vspace{8mm}
	        \large{Nishant Rai\textsuperscript{1}}\\
			\large{Sumit Negi\textsuperscript{2}}\\
			\large{Om Deshmukh\textsuperscript{2}}\\
			\vspace{3mm}
			
			{\normalsize{\textsuperscript{1} Indian Institute of Technology Kanpur\\}}
			{\normalsize{\textsuperscript{2} Xerox Research Centre India\\}}
	        \vspace{4mm}
        	\vspace{7mm}
	        \textbf{Abstract\\}
        	\vspace{4mm}
        	\noindent
{\justifying\small{The project deals with the problem of construction of Multiple Sense Embeddings for different words. We develop several models to tackle the problem consisting of Online Clustering Methods, Methods involving Parameter Estimation and also look at methods related to Word Word Co-occurrence matrices. The training data used is the April 2010 snapshot of the Wikipedia corpus~\cite{wiki}. Our model uses the popular Word2vec tool~\cite{word2vec} to compute Single Word Embeddings. We compare the performance of our approaches with current state of the art models~\cite{huang}~\cite{neel}and find that our model is comparable to the state of the art models and even outperforms them in some cases. Our model is extremely efficient and takes less than 6 hours for complete training and computation of senses. We also discuss the possibility of our model giving better (semantically coherent) senses than present models. The main task used for comparing the models is the SCWS task~\cite{huang}. The comparisons have been done using human judgment of semantic similarity.} \par}
			\vfill
            \vspace{3mm}
            \large\textsc{December '15 - Present\\}
            \vspace{1mm}
            \large\textsc{Xerox Research Centre India}
	    \end{center}
	\end{titlepage}
	\restoregeometry

	\tableofcontents

	\pagebreak	
	
	\section{Existing Work in NMF based Clustering}
	
	\subsection{Multi View Clustering using NMF}

The method was first proposed in ~\cite{nmfsdm}. The basic idea is as follows. For clustering, we assume that a data point in different views would be assigned to the same cluster with high probability. Therefore, in terms of matrix factorization, we require coefficient matrices learnt from different views to be softly regularize towards a common consensus. This consensus matrix is considered to reflect the latent clustering structure shared by different views.\\
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple view clustering using NMF. We adopt the following cost function (the rest of the procedure remains same as the standard NMF),
	\begin{multline*}	
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}\end{Vmatrix}^{2}_{F} \bigg) \\	
		\hfill {\text{s.t.}} \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\; \forall i \;\;s.t.\; 1 \leq i \leq n_{v}\\
		\hfill V^{*} \text{ is the consensus matrix}
	\end{multline*}	
	
	\subsection{Graph Regularized NMF}
	
	By using the non-negative constraints, NMF can learn a parts-based representation. However, NMF performs
this learning in the Euclidean space. It fails to discover the intrinsic geometrical and discriminating structure of the data space, which is essential to the real-world applications. The method was proposed in ~\cite{GReg} and has been used in many applications of NMF since then.	\\

	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple view clustering using NMF. We adopt the following cost function (the rest of the procedure remains same as the standard NMF),
	\begin{multline}	
	\hfill Loss = \big(\begin{Vmatrix} X-UV^{T} \end{Vmatrix}^{2}_{F}	
		+ \lambda Tr(V^{T} L V) \big) \;\; s.t. \hspace{2mm}  U\geq0,\; V\geq0,\;L\text{ : Graph Laplacian}
	\end{multline}	

	\section{Partial Multi Views}

Partial Multi View Clustering is a much more realistic model. Here, not all instances have complete views (i.e. do not have all sets of views). Which is highly likely for real world datasets. The setup remains roughly the same but with existence of partial views for instances. We discuss the existing work done in the field and propose different algorithms for the same.

	\subsection{Existing Methods}

	There has been relatively less work done in this area compared to Multi View Clustering. We discuss one of the models proposed in ~\cite{pvc15}. The basic idea is keeping a part of the view matrices common, which represents the common latent structure of the instances which have both views.\\
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for only 2 views (For simplicity), it can be extended to multiple views in a similar manner. We try to minimize the following cost function,
	\begin{multline*}
	Loss = \begin{Vmatrix} X_{1}-U_{1}V^{T}_{1} \end{Vmatrix}^{2}_{F}	
		+ \begin{Vmatrix} X_{2}-U_{2}V^{T}_{2} \end{Vmatrix}^{2}_{F}
		+ \lambda_{1} \Omega(V_{1}) + \lambda_{2} \Omega (V_{2})\\	
		s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq 2,
		\; \Omega \text{ represents the Lasso norm}
	\end{multline*}

	\subsection{Proposed Methods}
	
	We formulate new methods using extensions like Graph Regularization and varying Weights. The methods proposed and their details are given in the following sections.	
	
	\subsubsection{Graph Regularized PVC (Hard constraints)}
	
	The following method is inspired from ~\cite{pvc15}, ~\cite{Greg}. We use both hard constraints and graph regularization together to simultaneously respect the underlying intrinsic structure and also the common latent structure of the data.\\	
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for only 2 views (For simplicity), it can be extended to multiple views in a similar manner. We try to minimize the following cost function,
	\begin{multline}
	Loss = \begin{Vmatrix} X_{1}-U_{1}V^{T}_{1} \end{Vmatrix}^{2}_{F}	
		+ \begin{Vmatrix} X_{2}-U_{2}V^{T}_{2} \end{Vmatrix}^{2}_{F}
		+ \lambda_{2}Tr(V^{T}_{2}L_{2}V_{2}) + \lambda_{1}Tr(V^{T}_{1}L_{1}V_{1}) \\	
		s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq 2
	\end{multline}
	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $V_{c}$ with fixed $U_{v}$, $\overline{V_{v}}$
			\begin{multline}
			V_{i,j} = V_{i,j} \times \frac{(X_{\text{\tiny{c,1}}}^{\text{\small{T}}}U_{1}+X_{\text{\tiny{c,2}}}^{\text{\small{T}}}U_{2}+WV^{*})_{i,j}}{(V^{*}(U_{1}U_{1}^{\text{\small{T}}}+U_{2}U_{2}^{\text{\small{T}}})+DV^{*})_{i,j}}, \;\;\; where	\;\;\;
			\frac{D=\lambda_{1}D_{1}+\lambda_{2}D_{2}}{W=\lambda_{1}W_{1}+\lambda_{2}W_{2}}\\		
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \lambda_{2}}$, number of clusters $K$}
		\Output{Basis Matrices ${U_{1},U_{2}}$ and Coefficient Matrices ${V_{1},V_{2},V_{c}}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize  $U_{v}$, $\overline{V_{v}}$ by Eq. (1)\;
		Fix $U_{v}$, $\overline{V_{v}}$ update $V_{c}$ by Eq. (3)\;		
		\Repeat{Eq (2) converges}
		{
			\Repeat{Eq. (2) converges}
			{
				Fix $V_{c}$, update $U_{1}$, $U_{2}$, $\overline{V_{1}}$, $\overline{V_{2}}$ by Eq. (1)\;
			}
			Fix $U_{1}$, $U_{2}$, $\overline{V_{1}}$, $\overline{V_{2}}$ update $V_{c}$ by Eq. (3)\;
		}						
		\caption{Algorithm for minimizing the loss in Graph regularized PVC (Hard Constraints), given by Eq. (2). Based on Alternate Optimization}
	\end{algorithm}


	\subsubsection{Graph Regularized PVC (Soft constraints)}
	
	The following method is inspired from ~\cite{nmfsdm}, ~\cite{Greg}. It has been shown in ~\cite{nmfsdm} that applying soft constraints is better than hard constraints. Thus, we use soft constraints and graph regularization together to simultaneously respect the underlying intrinsic structure and also the common latent structure of the data.\\	
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple views. We adopt the following cost function,
	\begin{multline}	
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) \end{Vmatrix}^{2}_{F}
		+ \lambda_{i}Tr(V^{T}_{i}L_{i}V_{i}) \; \bigg)\\	
		\hfill s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq n_{v}\\
		\hfill V^{*} \text{ is the consensus matrix, }
		P_{i} \text{ represents the mapping of rows from } V_{i} \text{ to } V^{*}
	\end{multline}	

	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $U_{v}$, ${V_{v}}$ with fixed $V^{*}$ 
			\begin{multline}
			V_{i,j} = V_{i,j} \times \Bigg( \frac{(X^{\text{\small{T}}}U)_{i,j}+\lambda V^{*}_{{\textit{\small{I(i),j}}}} +\lambda \times (\mu WV)_{i,j}}{(V U^{\text{\small{T}}} U)_{i,j} + \lambda V_{i,j} +\lambda \times (\mu DV)_{i,j}} \Bigg) \\
			U_{i,j} = U_{i,j} \times \Bigg( \frac{(XV)_{i,j}
			+ \lambda  \sum_{\text{\tiny{t=1}}}^{\text{\tiny{N}}} 
			V_{t,j}V^{*}_\textit{\small{I(i),j}}}
			{(U V^{\text{\small{T}}} V)_{i,j} 
			+ \lambda \sum_{\text{\tiny{t=1}}}^{\text{\tiny{M}}} U_{t,k} 
			\sum_{\text{\tiny{s=1}}}^{\text{\tiny{N}}}V_{s,j}} \Bigg)
			\end{multline}
		\item \textsc{Rule 2:}	Update rule for updating $V^{*}$ with fixed $U_{v}$, ${V_{v}}$ 
			\begin{multline}
			V_{i}^{*} = \frac
			{\displaystyle \sum_{\textit{\small{(r,f)}} \in I^{-}(i)} \lambda_{f} (V_{f})_{r}}
			{\displaystyle \sum_{\textit{\small{(r,f)}} \in I^{-}(i)} \lambda_{f} }\\
			 I^{-}(i) : 
			\textsc{\small{Tuple of all the views which contain the }} i^{th} {\textsc{\small{ instance i.e. (View, Row)}}}
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \mu_{1}, \dots ,\lambda_{n_{v}},\mu_{n_{v}}}$, number of clusters $K$, mappings between the views and instances $P$}
		\Output{Basis Matrices ${U_{1}, \dots ,U_{n_{v}}}$, Coefficient Matrices ${V_{1}, \dots ,V_{n_{v}}}$ and Consensus Matrix $V^{*}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize  $U_{v}$, $V_{v}$ and $V^{*}$ by Eq. (1), (6)\;
		\Repeat{Eq (4) converges}
		{
			\For{$v\leftarrow 1$ \KwTo $n_{v}$}
			{
				\Repeat{Eq. (4) converges}
				{
					Fix $V^{*}$, update $U_{v}$, $V_{v}$ by Eq. (5)\;
				}
			}
			Fix $U_{v}$, $V_{v}$ update $V^{*}$ by Eq. (6)\;		
		}						
		\caption{Algorithm for minimizing the loss in Graph regularized PVC (Soft Constraints), given by Eq. (4). Based on Alternate Optimization}
	\end{algorithm}


	\subsubsection{Weighted Graph Regularized PVC (Soft constraints)}
	
	The following method is grossly similar to the previous one. The only difference being in the introduction of view weights. It is easy to realize that not all views are equally important, some might be not be important at all while others might be quite important. We introduce view weights as parameters to be learned during the optimization.\\	
	
	\noindent	
	\textbf{Formulation:}\\
	We explain the formulation for multiple views. We adopt the following cost function,
	\begin{multline}
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \alpha_{i}^{\gamma} \; 
		\bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) \end{Vmatrix}^{2}_{F}
		+ \lambda_{i}Tr(V^{T}_{i}L_{i}V_{i}) \; \bigg) \\	
		\hfill s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq n_{v}
		\;\; and \;\; \sum_{i=1}^{n_{v}} \alpha_{i} = 1 \\
		\hfill V^{*} \text{ is the consensus matrix, }
		P_{i} \text{ represents the mapping of rows from } V_{i} \text{ to } V^{*}
	\end{multline}
	
	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $U_{v}$, ${V_{v}}$ with fixed $V^{*}$, same as Eq. (5)
		\item \textsc{Rule 2:}	Update rule for updating $V^{*}$ with fixed $U_{v}$, ${V_{v}}$, same as Eq. (6) 
		\item \textsc{Rule 3:}	Update rule for updating $w_{v}$ with fixed $U_{v}$, ${V_{v}}$ and $V^{*}$ 
			\begin{multline}
			w_{v} = \frac{ \bigg( \gamma H_{v} \bigg)^{\frac{1}{1-\gamma}}}
			{\displaystyle \sum_{t=1}^{n_{v}}
			\bigg( \gamma H_{t} \bigg)^{\frac{1}{1-\gamma}}} \;\;\;
			where, \;\; H_{i} = \; \substack { \begin{Vmatrix} X_{i}-U_{i}V^{\text{\tiny{T}}}_{i}
			 \end{Vmatrix}^{\text{\tiny{2}}}_{\text{\tiny{F}}} + \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) 
			 \end{Vmatrix}^{\text{\tiny{2}}}_{\text{\tiny{F}}} \\
			+ \lambda_{i}Tr(V^{\text{\tiny{T}}}_{i}L_{i}V_{i}) }\\
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \mu_{1}, \dots ,\lambda_{n_{v}},\mu_{n_{v}}}$, number of clusters $K$, mappings between the views and instances $P$}
		\Output{Basis Matrices ${U_{1}, \dots ,U_{n_{v}}}$, Coefficient Matrices ${V_{1}, \dots ,V_{n_{v}}}$, View Weights ${w_{1}, \cdots ,w_{n_{v}}}$ and Consensus Matrix $V^{*}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize $w_{v}$ as $w_{v}=${\tiny{${\frac{1}{n_{v}}}$}}\;
		Initialize  $U_{v}$, $V_{v}$ and $V^{*}$ by Eq. (1), (6)\;
		\Repeat{Eq (7) converges}
		{
			\For{$v\leftarrow 1$ \KwTo $n_{v}$}
			{
				\Repeat{Eq. (7) converges}
				{
					Fix $V^{*}$, update $U_{v}$, $V_{v}$ by Eq. (5)\;
				}
			}
			Fix $U_{v}$, $V_{v}$ update $V^{*}$ by Eq. (6)\;		
			Update $w_{v}$ by Eq. (8)\;
		}						
		\caption{Algorithm for optimizing the given loss}
	\end{algorithm}

	\pagebreak

	\section{Partial View Experiment}	
	
	\subsection{Experiment Details}
	
	There are extremely few real partial view datasets. And even if we use them we'll be unable to analyse the effect of incomplete views on our models. Thus, we artificially create partial views as explained in the following sections.
	
	\subsubsection{Construction of Partial View datasets}	
To simulate the partial view setting, we randomly select a fraction of instances to be partial examples, i.e., they are described by only one of the views, and the remaining ones appear in all the views (i.e. they are complete).
	
	\subsubsection{Parameters Involved}	
	We denote the ratio of the incomplete instances to the total number of instances as PER (Partial Example Ratio) ~\cite{pvc15}. We vary PER from 10\% to 90\%, to get an idea of how our models perform in different conditions.	
	
	\subsubsection{Possible Improvements}
	
	Notice that here any instance has either all views or only one view. This is highly unlikely in any real world dataset. Thus, it can be said that the partial view instance generated is the worst possible dataset possible with the given value of PER (Worst in terms of the information carried with it).\\
	A slightly realistic way to generate the partial view instance would be to equally distribute the instances to all subsets i.e. if there are 3 views and 7 incomplete instances, then only 1 would have a single view, 3 would have two views and so on.
	
	\section{Results}

	\subsection{Evaluated Methods}
	
	We evaluate the performance of the following methods,
	\begin{itemize}
	\item {Partial Multi View Clustering (Li et al.) ~\cite{pvc15}}
	\item {Graph Regularized PVC (Hard constraints) (GPVC-A)	\small\textbf{(Proposed)}}
	\item {Graph Regularized PVC (Soft constraints) (GPVC-B)	\small\textbf{(Proposed)}}
	\item {Weighted Graph Regularized PVC (Soft constraints) (WGPVC)	\small\textbf{(Proposed)}}
	\end{itemize}

	\subsection{Datasets}
	
	The datasets used for evaluation are as follows,
	\begin{itemize}
	\item \textsc{3-Sources} : It is collected from three online news sources: BBC, Reuters, and The Guardian. In total there are 948 news articles covering 416 distinct news stories from the period February to April 2009. Of these stories, 169 were reported in all three sources. Each story was manually annotated with one of the six topical labels. We use the news reports common in any two sources in the experiments.
	\item \textsc{BBCSports} : Collection of sports news reports from BBC. Consists of 544 reports with 2 views each.
	\item \textsc{UCI Handwritten Digit dataset} : This hand-written digits (0-9) data is from the UCI repository.The dataset consists of 2000 examples, with view-1 being the 76 Fourier coefficients and view-2 being the 240 pixel averages in 2x3 windows.	
	\item \textsc{CiteSeer} : Collection of 3312 documents over 6 labels. It is made of 2 views (content, cites) on the same documents.
	\item \textsc{ORL} : Collection of facial images of 40 subjects. Consists of 400 images with 2 views each.	
	\end{itemize}

	\subsection{Evaluation Measures}

	The evaluation measures used are as follows,
	\begin{itemize}
	\item \textsc{Accuracy (ACC):} There is no obvious way of computing accuracy in case of clustering. Instead, we find the best map of the clusters to the classes (Using Hungarian Algorithm) and compute the measure.
	\item \textsc{Normalized Mutual Information (NMI):} Measure of how much knowing one thing can tell you about the other thing.
	\item \textsc{Purity (PUR):} Each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by the total samples.
	\end{itemize}

	\pagebreak	

    \newgeometry{left=1.5cm,right=1.5cm,top=3cm,bottom=3cm}

	\subsection{Plots}	
	
	\begin{figure}[H]
	
	\begin{tabular}[H]{ccc}
		\subfloat[Digit]{\includegraphics[width=5.5cm]{digitacc.jpg}} 
    	& \subfloat[Digit]{\includegraphics[width=5.5cm]{digitnmi.jpg}}
    	& \subfloat[Digit]{\includegraphics[width=5.5cm]{digitpur.jpg}}\\
	   
 	    \subfloat[ORL]{\includegraphics[width=5.5cm]{orlacc.jpg}} 
    	& \subfloat[ORL]{\includegraphics[width=5.5cm]{orlnmi.jpg}}
    	& \subfloat[ORL]{\includegraphics[width=5.5cm]{orlpur.jpg}}\\
	    
	    \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{csacc.jpg}} 
    	& \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{csnmi.jpg}}
    	& \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{cspur.jpg}}\\
	\end{tabular}
	\vspace{5mm}
	\caption*{\textsc{Results for varying PERs}}\label{foo}
	\end{figure}

	\restoregeometry	
	
	\pagebreak

	\subsection{Conclusions}

	\section{Varying Kernels}
	
	Observe that the graph Laplacian used plays a vital role in affecting the quality of the results. Thus, the way we construct the Graph Laplacian is extremely important. In the following sections, we discuss the major methods to construct them and also discuss some results and their implications.
	
	\subsection{Kernels Used}

	We define the various kernels used to construct the Graph Laplacian (The descriptions are taken from ~\cite{GReg}),
	\begin{itemize}
	\item \textsc{0-1 Weighting :} If nodes $j$ and $l$ are connected by an edge, put $W_{j,l} = 1$. This is the simplest weighting method and is very easy to compute. In our experiments we consider the nearest neighbor graph i.e. there is an edge between u, v if is one of the p nearest neighbors of u (We use p = 5).
	\item \textsc{Heat kernel weighting :} If nodes $j$ and $l$ are connected, put $W_{j,l} = e^{\frac{\|x_{j}-x_{l}\|^{\text{\tiny{2}}}}{\sigma}}$
	\item \textsc{Dot-product weighting :} If nodes $j$ and $l$ are connected, put $W_{j,l} = x^{T}_{j}x_{l}$. Note that, if x is normalized to 1, the dot product of two vectors is equivalent to the cosine similarity of the two vectors. Which is what we've used in our experiments.
	
	\end{itemize}	
	
	\subsection{Results}
	\subsubsection{Image Dataset : UCI Digit}
	\subsubsection{Text Dataset : BBCSports, 3Sources}
	
	\section{Weight Learning}
	
	\subsection{Intuition}
		
	The introduction of view weights is motivated by the observation that not all views are equally important. It is easy to realize that some views might be not be important at all while others might be quite important. We introduce view weights as parameters to be learned during the optimization. Hoping that the parameters learned would automatically give us the importance (goodness) of views.

	\subsection{Results}

	We show the plots of the performance of the Weighted Graph Regularized PVC example on different datasets. We also consider the best 2 views based on the weights, and check if they are truly the most informative views or not.	
	
	\subsubsection{Digit}
	\subsubsection{3Sources}
	\subsection{Conclusions}
	
	\section{Future Work}	
	
\end{document}


