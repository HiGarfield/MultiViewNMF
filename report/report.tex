\documentclass[a4paper]{article}
\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[export]{adjustbox}
\usepackage{enumerate}
\usepackage{url}
\usepackage{authblk}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[linesnumbered,ruled]{algorithm2e}

\graphicspath{{plots/}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\makeatletter
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
\makeatother

\DeclareMathSizes{10}{10}{10}{10}

\begin{document}

    \newgeometry{left=4cm,right=4cm,top=4cm,bottom=6cm}
	\begin{titlepage}
	    \begin{center}
	        \vspace*{1cm}
	        
	       	\huge{\textsc{Partial Multi View Clustering}}\\
			\Large{\textsc{Via Non Negative Matrix Factorization}}	

	        \vspace{8mm}
	        \large{Nishant Rai\textsuperscript{1}}\\
			\large{Sumit Negi\textsuperscript{2}}\\
			\large{Om Deshmukh\textsuperscript{2}}\\
			\vspace{3mm}
			
			{\normalsize{\textsuperscript{1} Indian Institute of Technology Kanpur\\}}
			{\normalsize{\textsuperscript{2} Xerox Research Centre India\\}}
	        \vspace{4mm}
        	\vspace{7mm}
	        \textbf{Abstract\\}
        	\vspace{4mm}
        	\noindent
{\justifying\small{The project deals with the problem of construction of Multiple Sense Embeddings for different words. We develop several models to tackle the problem consisting of Online Clustering Methods, Methods involving Parameter Estimation and also look at methods related to Word Word Co-occurrence matrices. The training data used is the April 2010 snapshot of the Wikipedia corpus~\cite{wiki}. Our model uses the popular Word2vec tool~\cite{word2vec} to compute Single Word Embeddings. We compare the performance of our approaches with current state of the art models~\cite{huang}~\cite{neel}and find that our model is comparable to the state of the art models and even outperforms them in some cases. Our model is extremely efficient and takes less than 6 hours for complete training and computation of senses. We also discuss the possibility of our model giving better (semantically coherent) senses than present models. The main task used for comparing the models is the SCWS task~\cite{huang}. The comparisons have been done using human judgment of semantic similarity.} \par}
			\vfill
            \vspace{3mm}
            \large\textsc{December '15 - Present\\}
            \vspace{1mm}
            \large\textsc{Xerox Research Centre India}
	    \end{center}
	\end{titlepage}
	\restoregeometry

	\tableofcontents

	\pagebreak	
	
	\section{Existing Work in NMF based Clustering}
	
	\subsection{Multi View Clustering using NMF}

The method was first proposed in ~\cite{nmfsdm}. The basic idea is as follows. For clustering, we assume that a data point in different views would be assigned to the same cluster with high probability. Therefore, in terms of matrix factorization, we require coefficient matrices learnt from different views to be softly regularize towards a common consensus. This consensus matrix is considered to reflect the latent clustering structure shared by different views.\\
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple view clustering using NMF. We adopt the following cost function (the rest of the procedure remains same as the standard NMF),
	\begin{multline*}	
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}\end{Vmatrix}^{2}_{F} \bigg) \\	
		\hfill {\text{s.t.}} \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\; \forall i \;\;s.t.\; 1 \leq i \leq n_{v}\\
		\hfill V^{*} \text{ is the consensus matrix}
	\end{multline*}	
	
	\subsection{Graph Regularized NMF}
	
	By using the non-negative constraints, NMF can learn a parts-based representation. However, NMF performs
this learning in the Euclidean space. It fails to discover the intrinsic geometrical and discriminating structure of the data space, which is essential to the real-world applications. The method was proposed in ~\cite{GReg} and has been used in many applications of NMF since then.	\\

	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple view clustering using NMF. We adopt the following cost function (the rest of the procedure remains same as the standard NMF),
	\begin{multline}	
	\hfill Loss = \big(\begin{Vmatrix} X-UV^{T} \end{Vmatrix}^{2}_{F}	
		+ \lambda Tr(V^{T} L V) \big) \;\; s.t. \hspace{2mm}  U\geq0,\; V\geq0,\;L\text{ : Graph Laplacian}
	\end{multline}	

	\section{Partial Multi Views}

Partial Multi View Clustering is a much more realistic model. Here, not all instances have complete views (i.e. do not have all sets of views). Which is highly likely for real world datasets. The setup remains roughly the same but with existence of partial views for instances. We discuss the existing work done in the field and propose different algorithms for the same.

	\subsection{Existing Methods}

	There has been relatively less work done in this area compared to Multi View Clustering. We discuss one of the models proposed in ~\cite{pvc15}. The basic idea is keeping a part of the view matrices common, which represents the common latent structure of the instances which have both views.\\
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for only 2 views (For simplicity), it can be extended to multiple views in a similar manner. We try to minimize the following cost function,
	\begin{multline*}
	Loss = \begin{Vmatrix} X_{1}-U_{1}V^{T}_{1} \end{Vmatrix}^{2}_{F}	
		+ \begin{Vmatrix} X_{2}-U_{2}V^{T}_{2} \end{Vmatrix}^{2}_{F}
		+ \lambda_{1} \Omega(V_{1}) + \lambda_{2} \Omega (V_{2})\\	
		s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq 2,
		\; \Omega \text{ represents the Lasso norm}
	\end{multline*}

	\subsection{Proposed Methods}
	
	We formulate new methods using extensions like Graph Regularization and varying Weights. The methods proposed and their details are given in the following sections.	
	
	\subsubsection{Graph Regularized PVC (Hard constraints)}
	
	The following method is inspired from ~\cite{pvc15}, ~\cite{Greg}. We use both hard constraints and graph regularization together to simultaneously respect the underlying intrinsic structure and also the common latent structure of the data.\\	
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for only 2 views (For simplicity), it can be extended to multiple views in a similar manner. We try to minimize the following cost function,
	\begin{multline}
	Loss = \begin{Vmatrix} X_{1}-U_{1}V^{T}_{1} \end{Vmatrix}^{2}_{F}	
		+ \begin{Vmatrix} X_{2}-U_{2}V^{T}_{2} \end{Vmatrix}^{2}_{F}
		+ \lambda_{2}Tr(V^{T}_{2}L_{2}V_{2}) + \lambda_{1}Tr(V^{T}_{1}L_{1}V_{1}) \\	
		s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq 2
	\end{multline}
	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $V_{c}$ with fixed $U_{v}$, $\overline{V_{v}}$
			\begin{multline}
			V_{i,j} = V_{i,j} \times \frac{(X_{\text{\tiny{c,1}}}^{\text{\small{T}}}U_{1}+X_{\text{\tiny{c,2}}}^{\text{\small{T}}}U_{2}+WV^{*})_{i,j}}{(V^{*}(U_{1}U_{1}^{\text{\small{T}}}+U_{2}U_{2}^{\text{\small{T}}})+DV^{*})_{i,j}}, \;\;\; where	\;\;\;
			\frac{D=\lambda_{1}D_{1}+\lambda_{2}D_{2}}{W=\lambda_{1}W_{1}+\lambda_{2}W_{2}}\\		
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \lambda_{2}}$, number of clusters $K$}
		\Output{Basis Matrices ${U_{1},U_{2}}$ and Coefficient Matrices ${V_{1},V_{2},V_{c}}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize  $U_{v}$, $\overline{V_{v}}$ by Eq. (1)\;
		Fix $U_{v}$, $\overline{V_{v}}$ update $V_{c}$ by Eq. (3)\;		
		\Repeat{Eq (2) converges}
		{
			\Repeat{Eq. (2) converges}
			{
				Fix $V_{c}$, update $U_{1}$, $U_{2}$, $\overline{V_{1}}$, $\overline{V_{2}}$ by Eq. (1)\;
			}
			Fix $U_{1}$, $U_{2}$, $\overline{V_{1}}$, $\overline{V_{2}}$ update $V_{c}$ by Eq. (3)\;
		}						
		\caption{Algorithm for minimizing the loss in Graph regularized PVC (Hard Constraints), given by Eq. (2). Based on Alternate Optimization}
	\end{algorithm}


	\subsubsection{Graph Regularized PVC (Soft constraints)}
	
	The following method is inspired from ~\cite{nmfsdm}, ~\cite{Greg}. It has been shown in ~\cite{nmfsdm} that applying soft constraints is better than hard constraints. Thus, we use soft constraints and graph regularization together to simultaneously respect the underlying intrinsic structure and also the common latent structure of the data.\\	
	
	\noindent
	\textbf{Formulation:}\\
	We explain the formulation for multiple views. We adopt the following cost function,
	\begin{multline}	
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) \end{Vmatrix}^{2}_{F}
		+ \lambda_{i}Tr(V^{T}_{i}L_{i}V_{i}) \; \bigg)\\	
		\hfill s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq n_{v}\\
		\hfill V^{*} \text{ is the consensus matrix, }
		P_{i} \text{ represents the mapping of rows from } V_{i} \text{ to } V^{*}
	\end{multline}	

	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $U_{v}$, ${V_{v}}$ with fixed $V^{*}$ 
			\begin{multline}
			V_{i,j} = V_{i,j} \times \Bigg( \frac{(X^{\text{\small{T}}}U)_{i,j}+\lambda V^{*}_{{\textit{\small{I(i),j}}}} +\lambda \times (\mu WV)_{i,j}}{(V U^{\text{\small{T}}} U)_{i,j} + \lambda V_{i,j} +\lambda \times (\mu DV)_{i,j}} \Bigg) \\
			U_{i,j} = U_{i,j} \times \Bigg( \frac{(XV)_{i,j}
			+ \lambda  \sum_{\text{\tiny{t=1}}}^{\text{\tiny{N}}} 
			V_{t,j}V^{*}_\textit{\small{I(i),j}}}
			{(U V^{\text{\small{T}}} V)_{i,j} 
			+ \lambda \sum_{\text{\tiny{t=1}}}^{\text{\tiny{M}}} U_{t,k} 
			\sum_{\text{\tiny{s=1}}}^{\text{\tiny{N}}}V_{s,j}} \Bigg)
			\end{multline}
		\item \textsc{Rule 2:}	Update rule for updating $V^{*}$ with fixed $U_{v}$, ${V_{v}}$ 
			\begin{multline}
			V_{i}^{*} = \frac
			{\displaystyle \sum_{\textit{\small{(r,f)}} \in I^{-}(i)} \lambda_{f} (V_{f})_{r}}
			{\displaystyle \sum_{\textit{\small{(r,f)}} \in I^{-}(i)} \lambda_{f} }\\
			 I^{-}(i) : 
			\textsc{\small{Tuple of all the views which contain the }} i^{th} {\textsc{\small{ instance i.e. (View, Row)}}}
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \mu_{1}, \dots ,\lambda_{n_{v}},\mu_{n_{v}}}$, number of clusters $K$, mappings between the views and instances $P$}
		\Output{Basis Matrices ${U_{1}, \dots ,U_{n_{v}}}$, Coefficient Matrices ${V_{1}, \dots ,V_{n_{v}}}$ and Consensus Matrix $V^{*}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize  $U_{v}$, $V_{v}$ and $V^{*}$ by Eq. (1), (6)\;
		\Repeat{Eq (4) converges}
		{
			\For{$v\leftarrow 1$ \KwTo $n_{v}$}
			{
				\Repeat{Eq. (4) converges}
				{
					Fix $V^{*}$, update $U_{v}$, $V_{v}$ by Eq. (5)\;
				}
			}
			Fix $U_{v}$, $V_{v}$ update $V^{*}$ by Eq. (6)\;		
		}						
		\caption{Algorithm for minimizing the loss in Graph regularized PVC (Soft Constraints), given by Eq. (4). Based on Alternate Optimization}
	\end{algorithm}


	\subsubsection{Weighted Graph Regularized PVC (Soft constraints)}
	
	The following method is grossly similar to the previous one. The only difference being in the introduction of view weights. It is easy to realize that not all views are equally important, some might be not be important at all while others might be quite important. We introduce view weights as parameters to be learned during the optimization.\\	
	
	\noindent	
	\textbf{Formulation:}\\
	We explain the formulation for multiple views. We adopt the following cost function,
	\begin{multline}
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \alpha_{i}^{\gamma} \; 
		\bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) \end{Vmatrix}^{2}_{F}
		+ \lambda_{i}Tr(V^{T}_{i}L_{i}V_{i}) \; \bigg) \\	
		\hfill s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq n_{v}
		\;\; and \;\; \sum_{i=1}^{n_{v}} \alpha_{i} = 1 \\
		\hfill V^{*} \text{ is the consensus matrix, }
		P_{i} \text{ represents the mapping of rows from } V_{i} \text{ to } V^{*}
	\end{multline}
	
	\noindent
	\textbf{Update Procedure:}
	\begin{itemize}
		\item \textsc{Rule 1:}	Update rule for updating $U_{v}$, ${V_{v}}$ with fixed $V^{*}$, same as Eq. (5)
		\item \textsc{Rule 2:}	Update rule for updating $V^{*}$ with fixed $U_{v}$, ${V_{v}}$, same as Eq. (6) 
		\item \textsc{Rule 3:}	Update rule for updating $w_{v}$ with fixed $U_{v}$, ${V_{v}}$ and $V^{*}$ 
			\begin{multline}
			w_{v} = \frac{ \bigg( \gamma H_{v} \bigg)^{\frac{1}{1-\gamma}}}
			{\displaystyle \sum_{t=1}^{n_{v}}
			\bigg( \gamma H_{t} \bigg)^{\frac{1}{1-\gamma}}} \;\;\;
			where, \;\; H_{i} = \; \substack { \begin{Vmatrix} X_{i}-U_{i}V^{\text{\tiny{T}}}_{i}
			 \end{Vmatrix}^{\text{\tiny{2}}}_{\text{\tiny{F}}} + \mu_{i}\begin{Vmatrix} V_{i}-V^{*}(\, P_{i} \, ) 
			 \end{Vmatrix}^{\text{\tiny{2}}}_{\text{\tiny{F}}} \\
			+ \lambda_{i}Tr(V^{\text{\tiny{T}}}_{i}L_{i}V_{i}) }\\
			\end{multline}
	\end{itemize}		
	
	\noindent
	\textbf{Pseudo Code:}	
	\begin{algorithm}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Nonnegative Matrix ${X_{1}, \dots ,X_{n_{v}}}$, parameters ${\lambda_{1}, \mu_{1}, \dots ,\lambda_{n_{v}},\mu_{n_{v}}}$, number of clusters $K$, mappings between the views and instances $P$}
		\Output{Basis Matrices ${U_{1}, \dots ,U_{n_{v}}}$, Coefficient Matrices ${V_{1}, \dots ,V_{n_{v}}}$, View Weights ${w_{1}, \cdots ,w_{n_{v}}}$ and Consensus Matrix $V^{*}$}
		Construct Graph Laplacians $L_{v}$ for each view\;
		Normalize each view $X_{v}$ such that $\|X_{v}\|_{\text{\tiny1}} = 1 $\;
		Initialize $w_{v}$ as $w_{v}=${\tiny{${\frac{1}{n_{v}}}$}}\;
		Initialize  $U_{v}$, $V_{v}$ and $V^{*}$ by Eq. (1), (6)\;
		\Repeat{Eq (7) converges}
		{
			\For{$v\leftarrow 1$ \KwTo $n_{v}$}
			{
				\Repeat{Eq. (7) converges}
				{
					Fix $V^{*}$, update $U_{v}$, $V_{v}$ by Eq. (5)\;
				}
			}
			Fix $U_{v}$, $V_{v}$ update $V^{*}$ by Eq. (6)\;		
			Update $w_{v}$ by Eq. (8)\;
		}						
		\caption{Algorithm for optimizing the given loss}
	\end{algorithm}

	\pagebreak

	\section{Results}

	\subsection{Methods}
	
	We evaluate the performance of the following methods,
	\begin{itemize}
	\item {Partial Multi View Clustering (Li et al.) ~\cite{pvc15}}
	\item {Graph Regularized PVC (Hard constraints) (GPVC-A)	\small\textbf{(Proposed)}}
	\item {Graph Regularized PVC (Soft constraints) (GPVC-B)	\small\textbf{(Proposed)}}
	\item {Weighted Graph Regularized PVC (Soft constraints) (WGPVC)	\small\textbf{(Proposed)}}
	\end{itemize}

	\subsection{Datasets}
	
	The datasets used for evaluation are as follows,
	\begin{itemize}
	\item \textsc{3-Sources} : It is collected from three online news sources: BBC, Reuters, and The Guardian. In total there are 948 news articles covering 416 distinct news stories from the period February to April 2009. Of these stories, 169 were reported in all three sources. Each story was manually annotated with one of the six topical labels. We use the news reports common in any two sources in the experiments.
	\item \textsc{BBCSports} : Collection of sports news reports from BBC. Consists of 544 reports with 2 views each.
	\item \textsc{UCI Handwritten Digit dataset} : This hand-written digits (0-9) data is from the UCI repository.The dataset consists of 2000 examples, with view-1 being the 76 Fourier coefficients and view-2 being the 240 pixel averages in 2 × 3 windows.	
	\item \textsc{CiteSeer} : Collection of 3312 documents over 6 labels. It is made of 2 views (content, cites) on the same documents.
	\item \textsc{ORL} : Collection of facial images of 40 subjects. Consists of 400 images with 2 views each.	
	\end{itemize}

	\subsection{Evaluation Measures}

	The evaluation measures used are as follows,
	\begin{itemize}
	\item {Normalised Mutual Information (NMI)}
	\item {Average Entropy (ENT)}
	\item {Adjusted Random Index (ARI)}
	\item {Purity (PR)}
	\item {Accuracy (ACC)}
	\end{itemize}
	
	\section{Partial View Experiment}	
	
	\subsection{Experiment Details}
	
	\subsubsection{Construction of Partial View datasets}	
	\subsubsection{Parameters Involved}	
	\subsubsection{Possible Improvements}
	
    \newgeometry{left=0.8cm,right=0.8cm,top=2cm,bottom=2cm}

	\subsection{Result Table}	

	\begin{table}[h!]
	  \begin{center}
	    \begin{tabular}{r|c|c|c|c|c}
	      \toprule
	      \textsc{DataSet} & \textsc{Method} & \textsc{NMI} & \textsc{Entropy} & \textsc{AdjustRI} & \textsc{Purity} 	\\
            
		  \midrule
		  & \textsc{GraphRegPVC} &	0.8347 (0.0165) & 0.5843 (0.0643) & 0.7515 (0.0364) & 0.8349 (0.0314)\\	  
		  & \textsc{(Hard)} &&&& \\	  
		  \textsc{Digit} &  
		  \textsc{GraphRegPVC} & \textbf{0.8892 (0.0256)} & \textbf{0.3959 (0.1099)} 
		  & \textbf{0.8371 (0.0638)} & \textbf{0.8975 (0.0527)}\\
		  & \textsc{(Soft)} &&&&\\	  
		  & \textsc{PartialMV} & 0.6274 (0.0180) & 1.2547 (0.0643) & 0.5192 (0.0311) & 0.6998 (0.0337)\\
		  & \textsc{(Hard)} &&&&\\
                        
	      \midrule
		  & \textsc{GraphRegPVC} & 0.5556 (0.0265) & 1.0095 (0.0690) & 0.4391 (0.0518) & 0.7329 (0.0286)\\
		  & \textsc{(Hard)} &&&&\\
		  \textsc{3 Sources (BG)} & 
		  \textsc{GraphRegPVC} & 0.5493 (0.0187) & 1.1643 (0.0492) & 0.4625 (0.0261) & 0.6746 (0.0156)\\
		  & \textsc{(Soft)} &&&&\\	  
		  & \textsc{PartialMV} & \textbf{0.6434 (0.0169)} & \textbf{0.9001 (0.0404)} 
		  & \textbf{0.6010 (0.0358)} & \textbf{0.7543 (0.0171)}\\
		  & \textsc{(Hard)} &&&&\\
		  
	      \midrule
		  & \textsc{GraphRegPVC} &	\textbf{0.5917 (0.0227)} & 1.0347 (0.0595) 
		  & \textbf{0.5018 (0.0420)} & 0.6991 (0.0168) \\	  
		  & \textsc{(Hard)} &&&&\\	              
		  \textsc{3 Sources (BR)} & 
		  \textsc{GraphRegPVC} & 0.5500 (0.0213) & 1.0444 (0.0628) & 0.4266 (0.0264) & 0.7108 (0.0196)\\
		  & \textsc{(Soft)} &&&&\\	  
		  & \textsc{PartialMV} & 0.5674 (0.0189) & \textbf{1.0076 (0.0461)} & 0.4626 (0.0366) 
		  & \textbf{0.7351 (0.0187)}\\
		  & \textsc{(Hard)} &&&&\\

	      \midrule
		  & \textsc{GraphRegPVC} &	0.5697 (0.0192) & 0.9923 (0.0450) & 0.4466 (0.0433) & 0.7252 (0.0166)\\	  
		  & \textsc{(Hard)} &&&& \\            
		  \textsc{3 Sources (GR)} &
		  \textsc{GraphRegPVC} & \textbf{0.5844 (0.0279)} & \textbf{0.9450 (0.0676)}
		  & \textbf{0.4940 (0.0450)} & \textbf{0.7452 (0.0284)}\\
		  & \textsc{(Soft)} &&&&\\	  
		  & \textsc{PartialMV} & 0.5467 (0.0303) & 1.0263 (0.0701) & 0.4546 (0.0547) & 0.7322 (0.0267)\\
		  & \textsc{(Hard)} &&&&\\
		  
		  \midrule
		  & \textsc{GraphRegPVC} &	\textbf{0.5294 (0.0167)} & \textbf{1.0270 (0.0316)}
		  & 0.4651 (0.0375) & 0.7014 (0.0161)\\	  
		  & \textsc{(Hard)} &&&& \\	  
		  \textsc{BBCSports} & 
		  \textsc{GraphRegPVC} & 0.4763 (0.0124) & 1.1405 (0.0347) & 0.4073 (0.0226) & 0.6856 (0.0082) \\
		  & \textsc{(Soft)} &&&& \\	  
		  & \textsc{PartialMV} & 0.5049 (0.0199) & 1.1006 (0.0391) 
		  & \textbf{0.4895 (0.0356)} & \textbf{0.7065 (0.0218)}\\
		  & \textsc{(Hard)} &&&&\\            
		  
		  \bottomrule
	    \end{tabular}
	    \caption{\textsc{Table for Results\\(For PER = 1.0)}}
	  \end{center}
	\end{table}		

	\restoregeometry	
	
	\pagebreak
			
    \newgeometry{left=1.5cm,right=1.5cm,top=3cm,bottom=3cm}

	\subsection{Plots}	
	
	\begin{figure}[H]
	
	\begin{tabular}[H]{ccc}
		\subfloat[Digit]{\includegraphics[width=5.5cm]{digitacc.jpg}} 
    	& \subfloat[Digit]{\includegraphics[width=5.5cm]{digitnmi.jpg}}
    	& \subfloat[Digit]{\includegraphics[width=5.5cm]{digitpur.jpg}}\\
	   
 	    \subfloat[ORL]{\includegraphics[width=5.5cm]{orlacc.jpg}} 
    	& \subfloat[ORL]{\includegraphics[width=5.5cm]{orlnmi.jpg}}
    	& \subfloat[ORL]{\includegraphics[width=5.5cm]{orlpur.jpg}}\\
	    
	    \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{csacc.jpg}} 
    	& \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{csnmi.jpg}}
    	& \subfloat[CiteSeer]{\includegraphics[width=5.5cm]{cspur.jpg}}\\
	\end{tabular}
	\vspace{5mm}
	\caption*{\textsc{Results for varying PERs}}\label{foo}
	\end{figure}

	\restoregeometry	
	
	\pagebreak

	\subsection{Conclusions}

	\section{Complete Multi Views}

	\subsection{Proposed Methods}
	
	The methods proposed and their details are given in the following sections.	
	
	\subsubsection{Weighted Graph Regularized PVC (Soft constraints)}
	
	\textsc{Formulation:}\\
	We explain the formulation for multiple views. We adopt the following cost function,
	\begin{align*}
	Loss = \displaystyle\sum_{i=1}^{n_{v}} \; \alpha_{i}^{\gamma} \; 
		\bigg(\begin{Vmatrix} X_{i}-U_{i}V^{T}_{i} \end{Vmatrix}^{2}_{F}	
		+ \mu_{i}\begin{Vmatrix} V_{i}-V^{*} \end{Vmatrix}^{2}_{F}
		+ \lambda_{i}Tr(V^{T}_{i}L_{i}V_{i}) \; \bigg)\\	
		s.t. \hspace{2mm}  U_{i}\geq0,\;V_{i}\geq0,\;V^{*}\geq0,\;\forall i \;\;s.t.\; 1 \leq i \leq n_{v}
		\;\; and \;\; \sum_{i=1}^{n_{v}} \alpha_{i} = 1 \\
		V^{*} \text{ is the consensus matrix, }
	\end{align*}
	\textsc{Update Procedure:}


	\subsection{Methods}
	
	The proposed methods are as follows,
	\begin{itemize}
	\item {Multi View Clustering (Liu et al.) (MultiNMF)}
	\item {Graph Regularized MultiNMF (GMultiNMF)}
	\item {Weighted Graph Regularized MultiNMF (WGNMF)	\small\textbf{(Proposed)}}
	\end{itemize}

	\subsection{Datasets}
	
	The datasets used for evaluation are as follows,
	\begin{itemize}
	\item \textsc{3-Sources} : It is collected from three online news sources: BBC, Reuters, and The Guardian. In total there are 948 news articles covering 416 distinct news stories from the period February to April 2009. Of these stories, 169 were reported in all three sources. Each story was manually annotated with one of the six labels. We use the news reports common in all sources in the experiments.
	\item \textsc{BBCSport} : Collection of sports news reports from BBC. Consists of 544 reports with 2 views each. Some reports have also 3-4 views in common. They are also used in the experiments.
	\item \textsc{UCI Handwritten Digit dataset} : This hand-written digits (0-9) data is from the UCI repository.The dataset consists of 2000 examples, with view-1 being the 76 Fourier coefficients and view-2 being the 240 pixel averages in 2 × 3 windows. All 6 provided views are also considered for experiments.
	\item \textsc{ORL} : Collection of facial images of 40 subjects. Consists of 400 images with 2 views each.
	\item \textsc{CiteSeer} : Collection of 3312 documents over 6 labels. It is made of 4 views (content, inbound, outbound, cites) on the same documents.
	\end{itemize}

	\subsection{Evaluation Measures}

	The evaluation measures used are as follows,
	\begin{itemize}
	\item {Accuracy (ACC)}
	\item {Normalised Mutual Information (NMI)}
	\item {Purity (PR)}
	\end{itemize}
	
    \newgeometry{left=0.8cm,right=0.8cm,top=2cm,bottom=2cm}

	\subsection{Result Table}	

	We show the results for the Digit dataset,	
	\begin{table}[h!]
	  \begin{center}
	    \begin{tabular}{c|c|c|c}
	      \toprule
	      \textsc{Method} & \textsc{Accuracy (\%) } & \textsc{NMI (\%) } 
	      & \textsc{Purity (\%)} 	\\
            
		  \midrule
			\textsc{BSV} & 68.5 & 63.4 & NA\\
			\textsc{WSV} & 63.5 & 60.3 & NA\\
			\textsc{ConcatNMF} & 67.8 & 60.3 & NA\\
			\textsc{ColNMF} & 66.0 & 62.1 & NA\\
			\textsc{Co-reguSC} & 86.6 & 77.0 & NA\\
			\textsc{MultiNMF} & 88.1 & 80.4 & NA\\
			\textsc{SC-ML} & 88.1 & 87.6 & NA\\
			\textsc{GPVC-S} & 95.1 & 90.1 & 95.1\\  
			\textsc{WPVC} & \textbf{96.8} & \textbf{93.2} & \textbf{96.8}\\
		  
		  \bottomrule
	    \end{tabular}
	    \caption*{\textsc{Results for Digit (2 views)}}
	  \end{center}
	\end{table}		

	We show the results for the multiple datasets, but compare only GPVC-S and WPVC,	
	\begin{table}[h!]
	  \begin{center}
	    \begin{tabular}{c|c|c|c|c}
	      \toprule
		  \textsc{Dataset} & \textsc{Method} & \textsc{Accuracy (\%) } & \textsc{NMI (\%) } 
	      & \textsc{Purity (\%)} 	\\
            
		  \midrule
			\textsc{Digit} & \textsc{GPVC-S} & 73.3 & 68.6 & 73.3\\  
			\footnotesize\textsc{(6 Views)} & \textsc{WPVC} & \textbf{93.0} & \textbf{87.5} & \textbf{93.0}\\
			& \textsc{MVC} & 86.3 & 78.1 & 86.3\\  
		  \midrule
	    	\textsc{3Sources} & \textsc{GPVC-S} & 62.7 & 54.5 & 68.6\\  
			\footnotesize\textsc{(3 Views)} & \textsc{WPVC} & \textbf{63.2} & \textbf{57.1} & \textbf{69.2}\\
			& \textsc{MVC} & 46.7 & 37.1 & 61.5\\  
		  \midrule
	    	\textsc{BBCSports} & \textsc{GPVC-S} & \textbf{59.2} & \textbf{26.2} & \textbf{59.6}\\  
			\footnotesize\textsc{(3 Views)} & \textsc{WPVC} & {57.4} & {21.7} & {57.8}\\
			& \textsc{MVC} & 53.2 & 23.4 & 53.9\\  
		  \midrule

	    \end{tabular}
	    \caption*{\textsc{Results for Digit (2 views)}}
	  \end{center}
	\end{table}		

	\restoregeometry	

	\section{Varying Kernels}
	
	\subsection{Kernels Used}
	\subsection{Results}
	\subsubsection{Image Dataset : UCI Digit}
	\subsubsection{Text Dataset : BBCSports, 3Sources}
	
	\section{Weight Learning}
	\subsection{Intuition}
	\subsection{Results}
	\subsubsection{Digit}
	\subsubsection{3Sources}
	\subsection{Conclusions}
	
\end{document}


